{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xmKDgeXE2Gw"
      },
      "source": [
        "!pip install spacy -U\r\n",
        "!python -m spacy download en_core_web_lg\r\n",
        "!pip install lemminflect\r\n",
        "import spacy\r\n",
        "import lemminflect\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "try:\r\n",
        "    nlp = spacy.load(\"en_core_web_lg\", disable = [\"parser\", \"ner\"])\r\n",
        "except OSError:\r\n",
        "    print(\r\n",
        "        \"The runtime needs to be restarted otherwise COLAB is unable to find the just installed model.\"\r\n",
        "    )\r\n",
        "    exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxUgcsIGE3uo"
      },
      "source": [
        "if \"google.colab\" in str(get_ipython()):\r\n",
        "    COLAB = True\r\n",
        "    !git clone https://github.com/exc4l/eigolingo\r\n",
        "    eigodir = \"eigolingo/\"\r\n",
        "    drivedir = \"eigolingo/lists/\"\r\n",
        "    from google.colab import files\r\n",
        "else:\r\n",
        "    COLAB = False\r\n",
        "    eigodir = \"./\"\r\n",
        "    drivedir = \"lists/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Ps7pWbE6C2"
      },
      "source": [
        "# supplements\r\n",
        "with open(\r\n",
        "    drivedir + \"en-drv-1610-douay-rheims-bible-1-0.dic\", \"r\", encoding=\"utf-8\"\r\n",
        ") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "bible1set = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(\r\n",
        "    drivedir + \"en-kjv-1611-king-james-bible-1-0.dic\", \"r\", encoding=\"utf-8\"\r\n",
        ") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "bible2set = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "bibleset = bible1set.union(bible2set)\r\n",
        "with open(drivedir + \"chemistry.dic\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "chemset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(drivedir + \"medterms.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "medset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "\r\n",
        "with open(drivedir + \"engterms.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "engset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "\r\n",
        "with open(\r\n",
        "    drivedir + \"wordlist_marcoagpinto_20210301_252214w.txt\", \"r\", encoding=\"utf-8\"\r\n",
        ") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "marcoset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "\r\n",
        "supset = set.union(*[bibleset, chemset, medset, engset, marcoset])\r\n",
        "print(len(supset))\r\n",
        "\r\n",
        "with open(drivedir + \"names.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().lower().splitlines()\r\n",
        "nameset = set(data)\r\n",
        "\r\n",
        "# remove names\r\n",
        "for ele in nameset:\r\n",
        "    if ele.lower() in supset:\r\n",
        "        supset.remove(ele.lower())\r\n",
        "# user defined list of additional entries\r\n",
        "with open(drivedir + \"add_entries.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "extraset = {w for w in data}\r\n",
        "\r\n",
        "supset = supset.union(extraset)\r\n",
        "\r\n",
        "print(len(supset))\r\n",
        "\r\n",
        "# countries and cities and additional (will be removed before saving)\r\n",
        "with open(drivedir + \"countries.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "countries = {w.lower() for w in data}\r\n",
        "with open(drivedir + \"cities.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "cities = {w.lower() for w in data}\r\n",
        "with open(drivedir + \"additional_removal.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "addiset = {w.lower() for w in data}\r\n",
        "geoset = set.union(*[countries, cities, addiset])\r\n",
        "print(len(geoset))\r\n",
        "\r\n",
        "\r\n",
        "# filtering certain letters before processing in spacy\r\n",
        "with open(drivedir + \"letters_to_filter.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    letters_to_filter = f.read().replace(\"\\n\", \"\")\r\n",
        "filtertrans = str.maketrans(\"\", \"\", letters_to_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_o9ZFvqFmNX"
      },
      "source": [
        "# main corpus\r\n",
        "with open(drivedir + \"create95hacknoroman.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "    data = data[44:]\r\n",
        "c95 = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(drivedir + \"create80hacknoroman.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "    data = data[44:]\r\n",
        "c80 = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(drivedir + \"create70hacknoroman.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "    data = data[44:]\r\n",
        "c70 = {w for w in data if w.islower() and w.isalpha()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFGqCDU6FrGZ"
      },
      "source": [
        "prepset = c95.union(supset)\r\n",
        "prepset = prepset.difference(geoset)\r\n",
        "for ele in list(prepset):\r\n",
        "    if ele != ele.translate(filtertrans):\r\n",
        "        print(ele)\r\n",
        "        prepset.remove(ele)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D9ZiAOnFtIZ"
      },
      "source": [
        "# consider the dict in the repository\r\n",
        "def read_dict(textlines):\r\n",
        "    rdict = dict()\r\n",
        "    for line in textlines:\r\n",
        "        key, value = line.split(\":\")\r\n",
        "        rdict[key] = value\r\n",
        "    return rdict\r\n",
        "\r\n",
        "\r\n",
        "with open(eigodir + \"dict95.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    prev_dict = read_dict(f.read().splitlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUwChwfgGWml"
      },
      "source": [
        "def get_testcases(input):\r\n",
        "    return [f\"{input}\", f\"abc {input}\", f\"{input} abc\", f\"abc {input} abc\"]\r\n",
        "\r\n",
        "\r\n",
        "resulting_dict = dict()\r\n",
        "for ele in tqdm(prepset):\r\n",
        "    if ele in resulting_dict:\r\n",
        "        continue\r\n",
        "    if ele in prev_dict:\r\n",
        "        resulting_dict[ele] = prev_dict[ele]\r\n",
        "        continue\r\n",
        "    results = set()\r\n",
        "    for w in get_testcases(ele):\r\n",
        "        doc = nlp(w)\r\n",
        "        for token in doc:\r\n",
        "            if token.text != \"abc\":\r\n",
        "                results.add(token._.lemma())\r\n",
        "    if not results:\r\n",
        "        continue\r\n",
        "    res = min(results, key=len)\r\n",
        "    if res == ele:\r\n",
        "        if len(results) > 1:\r\n",
        "            results.remove(res)\r\n",
        "            res = min(results, key=len)\r\n",
        "            resulting_dict[ele] = res\r\n",
        "        else:\r\n",
        "            resulting_dict[ele] = res\r\n",
        "    else:\r\n",
        "        resulting_dict[ele] = res\r\n",
        "for ele in tqdm(list(resulting_dict.values())):\r\n",
        "    if ele not in resulting_dict and ele not in geoset and ele:\r\n",
        "        resulting_dict[ele] = ele\r\n",
        "if \"\" in resulting_dict:\r\n",
        "    del resulting_dict[\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNWspTFMOICk"
      },
      "source": [
        "with open(\"dict95.txt\", \"w\", encoding=\"utf-8\") as wr:\r\n",
        "    for k in sorted(list(resulting_dict.keys())):\r\n",
        "        wr.write(f\"{k}:{resulting_dict[k]}\\n\")\r\n",
        "if COLAB:\r\n",
        "    files.download(\"dict95.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctVvlX_pOzA5"
      },
      "source": [
        "print(\"New:\\n\")\r\n",
        "for key in resulting_dict.keys():\r\n",
        "    if key not in prev_dict:\r\n",
        "        print(key)\r\n",
        "print(\"\\nRemoved:\\n\")\r\n",
        "for key in prev_dict.keys():\r\n",
        "    if key not in resulting_dict:\r\n",
        "        print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjTelGT7PgYY"
      },
      "source": [
        "# generate dict70, 80\r\n",
        "prepset = c80.union(supset)\r\n",
        "prepset = prepset.difference(geoset)\r\n",
        "for ele in list(prepset):\r\n",
        "    if ele != ele.translate(filtertrans):\r\n",
        "        prepset.remove(ele)\r\n",
        "\r\n",
        "resulting_dict80 = dict()\r\n",
        "for ele in tqdm(prepset):\r\n",
        "    if ele == \"\":\r\n",
        "        print(\"yep\")\r\n",
        "    if ele in resulting_dict:\r\n",
        "        resulting_dict80[ele] = resulting_dict[ele]\r\n",
        "for ele in tqdm(list(resulting_dict80.values())):\r\n",
        "    if ele not in resulting_dict80 and ele not in geoset and ele:\r\n",
        "        resulting_dict80[ele] = ele\r\n",
        "\r\n",
        "prepset = c70.union(supset)\r\n",
        "prepset = prepset.difference(geoset)\r\n",
        "for ele in list(prepset):\r\n",
        "    if ele != ele.translate(filtertrans):\r\n",
        "        prepset.remove(ele)\r\n",
        "\r\n",
        "resulting_dict70 = dict()\r\n",
        "for ele in tqdm(prepset):\r\n",
        "    if ele in resulting_dict:\r\n",
        "        resulting_dict70[ele] = resulting_dict[ele]\r\n",
        "for ele in tqdm(list(resulting_dict70.values())):\r\n",
        "    if ele not in resulting_dict70 and ele not in geoset and ele:\r\n",
        "        resulting_dict70[ele] = ele\r\n",
        "\r\n",
        "with open(\"dict80.txt\", \"w\", encoding=\"utf-8\") as wr:\r\n",
        "    for k in sorted(list(resulting_dict80.keys())):\r\n",
        "        wr.write(f\"{k}:{resulting_dict[k]}\\n\")\r\n",
        "with open(\"dict70.txt\", \"w\", encoding=\"utf-8\") as wr:\r\n",
        "    for k in sorted(list(resulting_dict70.keys())):\r\n",
        "        wr.write(f\"{k}:{resulting_dict[k]}\\n\")\r\n",
        "if COLAB:\r\n",
        "    files.download(\"dict80.txt\")\r\n",
        "    files.download(\"dict70.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emOU6b2REcr"
      },
      "source": [
        "# generate wordlists\r\n",
        "wl70 = sorted(list(resulting_dict70.keys()))\r\n",
        "wl80 = sorted(list(resulting_dict80.keys()))\r\n",
        "wl95 = sorted(list(resulting_dict.keys()))\r\n",
        "\r\n",
        "\r\n",
        "def write_list(filename, datalist):\r\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as wr:\r\n",
        "        wr.write(\"\\n\".join(datalist))\r\n",
        "\r\n",
        "\r\n",
        "write_list(\"wordlist70.txt\", wl70)\r\n",
        "write_list(\"wordlist80.txt\", wl80)\r\n",
        "write_list(\"wordlist95.txt\", wl95)\r\n",
        "\r\n",
        "if COLAB:\r\n",
        "    files.download(\"wordlist70.txt\")\r\n",
        "    files.download(\"wordlist80.txt\")\r\n",
        "    files.download(\"wordlist95.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
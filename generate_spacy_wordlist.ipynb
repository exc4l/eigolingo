{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_spacy_wordlist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (Spyder)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSYAjWohijiU"
      },
      "source": [
        "!pip install spacy\r\n",
        "!python -m spacy download en_core_web_lg\r\n",
        "import spacy\r\n",
        "from tqdm import tqdm\r\n",
        "try:\r\n",
        "    nlp = spacy.load(\"en_core_web_lg\", disable = [\"parser\", \"ner\"])\r\n",
        "except OSError:\r\n",
        "    print(\"The runtime needs to be restarted otherwise COLAB is unable to find the just installed model.\")\r\n",
        "    exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7uHGQJf9tOP"
      },
      "source": [
        "if \"google.colab\" in str(get_ipython()):\r\n",
        "    COLAB = True\r\n",
        "    !git clone https://github.com/exc4l/eigolingo\r\n",
        "    drivedir = \"eigolingo/lists/\"\r\n",
        "    from google.colab import files\r\n",
        "else:\r\n",
        "    COLAB = False\r\n",
        "    drivedir = \"./\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnAQeMXhi810"
      },
      "source": [
        "# supplements\r\n",
        "with open(drivedir+\"en-drv-1610-douay-rheims-bible-1-0.dic\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "bible1set = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(drivedir+\"en-kjv-1611-king-james-bible-1-0.dic\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "bible2set = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "bibleset = bible1set.union(bible2set)\r\n",
        "with open(drivedir+\"chemistry.dic\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "chemset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(drivedir+\"medterms.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "medset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "\r\n",
        "with open(drivedir+\"engterms.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "engset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "\r\n",
        "supset = set.union(*[bibleset, chemset, medset, engset])\r\n",
        "print(len(supset))\r\n",
        "\r\n",
        "with open(drivedir+\"names.txt\", 'r', encoding='utf-8') as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "nameset = set(data)\r\n",
        "\r\n",
        "# remove names\r\n",
        "dellist = list()\r\n",
        "for ele in nameset:\r\n",
        "    if ele.lower() in supset:\r\n",
        "        dellist.append(ele.lower())\r\n",
        "dellist=set(dellist)\r\n",
        "for ele in dellist:\r\n",
        "    supset.remove(ele)\r\n",
        "print(len(supset))\r\n",
        "\r\n",
        "# countries and cities and additional (will be removed before saving)\r\n",
        "with open(drivedir+\"countries.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "countries = {w for w in data}\r\n",
        "with open(drivedir+\"cities.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "cities = {w for w in data}\r\n",
        "with open(drivedir+\"additional_removal.txt\", \"r\", encoding=\"utf-8\") as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "addiset = {w for w in data}\r\n",
        "geoset = set.union(*[countries, cities, addiset])\r\n",
        "print(len(geoset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Fz3M4vkTIT"
      },
      "source": [
        "# main corpus\r\n",
        "with open(drivedir+\"create95hacknoroman.txt\", 'r', encoding='utf-8') as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "    data = data[44:]\r\n",
        "c95 = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(drivedir+\"create80hacknoroman.txt\", 'r', encoding='utf-8') as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "    data = data[44:]\r\n",
        "c80 = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "with open(drivedir+\"create70hacknoroman.txt\", 'r', encoding='utf-8') as f:\r\n",
        "    data = f.read().splitlines()\r\n",
        "    data = data[44:]\r\n",
        "c70 = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "# some words seem to occur both in upper case and in lower case e.g. Europe and europe\r\n",
        "# filter upper case words that appear in lower case\r\n",
        "# cset = {w for w in data if w.islower() and w.isalpha()}\r\n",
        "# upset = {w for w in data if not w.islower() and w.isalpha()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BvtI3-ImjPn"
      },
      "source": [
        "prepset = c95.union(supset)\r\n",
        "addlist = list()\r\n",
        "for ele in tqdm(prepset):\r\n",
        "    doc = nlp(ele)\r\n",
        "    for token in doc:\r\n",
        "        addlist.append(token.lemma_)\r\n",
        "addset =  {w for w in addlist if w.islower() and w.isalpha()}\r\n",
        "finalset = prepset.union(addset)\r\n",
        "# remove countries and cities\r\n",
        "for loc in geoset:\r\n",
        "    if loc.lower() in finalset:\r\n",
        "        finalset.remove(loc.lower())\r\n",
        "        # print(loc.lower())\r\n",
        "print(len(finalset))\r\n",
        "with open(\"wordlist95.txt\", 'w', encoding='utf-8') as wr:\r\n",
        "    wr.write(\"\\n\".join(sorted(list(finalset))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qcHcwxxmny5"
      },
      "source": [
        "prepset = c80.union(supset)\r\n",
        "addlist = list()\r\n",
        "for ele in tqdm(prepset):\r\n",
        "    doc = nlp(ele)\r\n",
        "    for token in doc:\r\n",
        "        addlist.append(token.lemma_)\r\n",
        "addset =  {w for w in addlist if w.islower() and w.isalpha()}\r\n",
        "finalset = prepset.union(addset)\r\n",
        "# remove countries and cities\r\n",
        "for loc in geoset:\r\n",
        "    if loc.lower() in finalset:\r\n",
        "        finalset.remove(loc.lower())\r\n",
        "        # print(loc.lower())\r\n",
        "print(len(finalset))\r\n",
        "with open(\"wordlist80.txt\", 'w', encoding='utf-8') as wr:\r\n",
        "    wr.write(\"\\n\".join(sorted(list(finalset))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJb6BeW7njEy"
      },
      "source": [
        "prepset = c70.union(supset)\r\n",
        "addlist = list()\r\n",
        "for ele in tqdm(prepset):\r\n",
        "    doc = nlp(ele)\r\n",
        "    for token in doc:\r\n",
        "        addlist.append(token.lemma_)\r\n",
        "addset =  {w for w in addlist if w.islower() and w.isalpha()}\r\n",
        "finalset = prepset.union(addset)\r\n",
        "# remove countries and cities\r\n",
        "for loc in geoset:\r\n",
        "    if loc.lower() in finalset:\r\n",
        "        finalset.remove(loc.lower())\r\n",
        "        # print(loc.lower())\r\n",
        "print(len(finalset))\r\n",
        "with open(\"wordlist70.txt\", 'w', encoding='utf-8') as wr:\r\n",
        "    wr.write(\"\\n\".join(sorted(list(finalset))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca8mHxpcnje9"
      },
      "source": [
        "if COLAB:\r\n",
        "    files.download('wordlist70.txt') \r\n",
        "    files.download('wordlist80.txt')\r\n",
        "    files.download('wordlist95.txt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0A1MTCjwHMe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK47Mb79xBh5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D97csM3wxCf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoSwa7q0yLRs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLqEZtm7C65p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOYp1DyhDfvx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
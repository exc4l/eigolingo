{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "generate_spacy_wordlist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xmKDgeXE2Gw"
      },
      "source": [
        "!pip install spacy -U\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install lemminflect\n",
        "import spacy\n",
        "import lemminflect\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_lg\", disable = [\"parser\", \"ner\"])\n",
        "except OSError:\n",
        "    print(\n",
        "        \"The runtime needs to be restarted otherwise COLAB is unable to find the just installed model.\"\n",
        "    )\n",
        "    exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxUgcsIGE3uo"
      },
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    COLAB = True\n",
        "    !git clone https://github.com/exc4l/eigolingo\n",
        "    eigodir = \"eigolingo/\"\n",
        "    drivedir = \"eigolingo/lists/\"\n",
        "    from google.colab import files\n",
        "else:\n",
        "    COLAB = False\n",
        "    eigodir = \"./\"\n",
        "    drivedir = \"lists/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oN9xBC3n_6O"
      },
      "source": [
        "def get_testcases(input):\n",
        "    return [f\"{input}\", f\"abc {input}\", f\"{input} abc\", f\"abc {input} abc\"]\n",
        "\n",
        "def read_dict(textlines):\n",
        "    rdict = dict()\n",
        "    for line in textlines:\n",
        "        key, value = line.split(\":\")\n",
        "        rdict[key] = value\n",
        "    return rdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Ps7pWbE6C2"
      },
      "source": [
        "# supplements\n",
        "with open(\n",
        "    drivedir + \"en-drv-1610-douay-rheims-bible-1-0.dic\", \"r\", encoding=\"utf-8\"\n",
        ") as f:\n",
        "    data = f.read().splitlines()\n",
        "bible1set = {w for w in data if w.islower() and w.isalpha()}\n",
        "with open(\n",
        "    drivedir + \"en-kjv-1611-king-james-bible-1-0.dic\", \"r\", encoding=\"utf-8\"\n",
        ") as f:\n",
        "    data = f.read().splitlines()\n",
        "bible2set = {w for w in data if w.islower() and w.isalpha()}\n",
        "bibleset = bible1set.union(bible2set)\n",
        "with open(drivedir + \"chemistry.dic\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "chemset = {w for w in data if w.islower() and w.isalpha()}\n",
        "with open(drivedir + \"medterms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "medset = {w for w in data if w.islower() and w.isalpha()}\n",
        "\n",
        "with open(drivedir + \"engterms.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "engset = {w for w in data if w.islower() and w.isalpha()}\n",
        "\n",
        "with open(\n",
        "    drivedir + \"wordlist_marcoagpinto_20210301_252214w.txt\", \"r\", encoding=\"utf-8\"\n",
        ") as f:\n",
        "    data = f.read().splitlines()\n",
        "marcoset = {w for w in data if w.islower() and w.isalpha()}\n",
        "\n",
        "supset = set.union(*[bibleset, chemset, medset, engset, marcoset])\n",
        "print(len(supset))\n",
        "\n",
        "with open(drivedir + \"names.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().lower().splitlines()\n",
        "nameset = set(data)\n",
        "\n",
        "# remove names\n",
        "for ele in nameset:\n",
        "    if ele.lower() in supset:\n",
        "        supset.remove(ele.lower())\n",
        "# user defined list of additional entries\n",
        "with open(drivedir + \"add_entries.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "extraset = {w for w in data}\n",
        "\n",
        "supset = supset.union(extraset)\n",
        "\n",
        "print(len(supset))\n",
        "\n",
        "# countries and cities and additional (will be removed before saving)\n",
        "with open(drivedir + \"countries.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "countries = {w.lower() for w in data}\n",
        "with open(drivedir + \"cities.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "cities = {w.lower() for w in data}\n",
        "with open(drivedir + \"additional_removal.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "addiset = {w.lower() for w in data}\n",
        "geoset = set.union(*[countries, cities, addiset])\n",
        "print(len(geoset))\n",
        "\n",
        "\n",
        "# filtering certain letters before processing in spacy\n",
        "with open(drivedir + \"letters_to_filter.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    letters_to_filter = f.read().replace(\"\\n\", \"\")\n",
        "filtertrans = str.maketrans(\"\", \"\", letters_to_filter)\n",
        "\n",
        "# change certain dictionary entries before saving:\n",
        "with open(drivedir + \"change_dictionary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    change_dict = read_dict(f.read().splitlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_o9ZFvqFmNX"
      },
      "source": [
        "# main corpus\n",
        "with open(drivedir + \"create95hacknoroman.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "    data = data[44:]\n",
        "c95 = {w for w in data if w.islower() and w.isalpha()}\n",
        "with open(drivedir + \"create80hacknoroman.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "    data = data[44:]\n",
        "c80 = {w for w in data if w.islower() and w.isalpha()}\n",
        "with open(drivedir + \"create70hacknoroman.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read().splitlines()\n",
        "    data = data[44:]\n",
        "c70 = {w for w in data if w.islower() and w.isalpha()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFGqCDU6FrGZ"
      },
      "source": [
        "prepset = c95.union(supset)\n",
        "prepset = prepset.difference(geoset)\n",
        "for ele in list(prepset):\n",
        "    if ele != ele.translate(filtertrans):\n",
        "        print(ele)\n",
        "        prepset.remove(ele)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D9ZiAOnFtIZ"
      },
      "source": [
        "# consider the dict in the repository\n",
        "with open(eigodir + \"dict70.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    prev_dict = read_dict(f.read().splitlines())\n",
        "with open(eigodir + \"dict80.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    prev_dict.update(read_dict(f.read().splitlines()))\n",
        "with open(eigodir + \"dict95.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    prev_dict.update(read_dict(f.read().splitlines()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUwChwfgGWml"
      },
      "source": [
        "resulting_dict = dict()\n",
        "for ele in tqdm(prepset):\n",
        "    if ele in resulting_dict:\n",
        "        continue\n",
        "    if ele in prev_dict:\n",
        "        resulting_dict[ele] = prev_dict[ele]\n",
        "        continue\n",
        "    results = set()\n",
        "    for w in get_testcases(ele):\n",
        "        doc = nlp(w)\n",
        "        for token in doc:\n",
        "            if token.text != \"abc\":\n",
        "                results.add(token._.lemma())\n",
        "    if not results:\n",
        "        continue\n",
        "    res = min(results, key=len)\n",
        "    if res == ele:\n",
        "        if len(results) > 1:\n",
        "            results.remove(res)\n",
        "            res = min(results, key=len)\n",
        "            resulting_dict[ele] = res\n",
        "        else:\n",
        "            resulting_dict[ele] = res\n",
        "    else:\n",
        "        resulting_dict[ele] = res\n",
        "for ele in tqdm(list(resulting_dict.values())):\n",
        "    if ele not in resulting_dict and ele not in geoset and ele:\n",
        "        resulting_dict[ele] = ele\n",
        "if \"\" in resulting_dict:\n",
        "    del resulting_dict[\"\"]\n",
        "\n",
        "for key, val in change_dict.items():\n",
        "    if key in resulting_dict:\n",
        "        resulting_dict[key] = val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctVvlX_pOzA5"
      },
      "source": [
        "print(\"New:\\n\")\n",
        "for key in resulting_dict.keys():\n",
        "    if key not in prev_dict:\n",
        "        print(key)\n",
        "print(\"\\nRemoved:\\n\")\n",
        "for key in prev_dict.keys():\n",
        "    if key not in resulting_dict:\n",
        "        print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjTelGT7PgYY"
      },
      "source": [
        "# generate dict70, 80\n",
        "prepset = c80.union(supset)\n",
        "prepset = prepset.difference(geoset)\n",
        "for ele in list(prepset):\n",
        "    if ele != ele.translate(filtertrans):\n",
        "        prepset.remove(ele)\n",
        "\n",
        "resulting_dict80 = dict()\n",
        "for ele in tqdm(prepset):\n",
        "    if ele == \"\":\n",
        "        print(\"yep\")\n",
        "    if ele in resulting_dict:\n",
        "        resulting_dict80[ele] = resulting_dict[ele]\n",
        "for ele in tqdm(list(resulting_dict80.values())):\n",
        "    if ele not in resulting_dict80 and ele not in geoset and ele:\n",
        "        resulting_dict80[ele] = ele\n",
        "\n",
        "prepset = c70.union(supset)\n",
        "prepset = prepset.difference(geoset)\n",
        "for ele in list(prepset):\n",
        "    if ele != ele.translate(filtertrans):\n",
        "        prepset.remove(ele)\n",
        "\n",
        "resulting_dict70 = dict()\n",
        "for ele in tqdm(prepset):\n",
        "    if ele in resulting_dict:\n",
        "        resulting_dict70[ele] = resulting_dict[ele]\n",
        "for ele in tqdm(list(resulting_dict70.values())):\n",
        "    if ele not in resulting_dict70 and ele not in geoset and ele:\n",
        "        resulting_dict70[ele] = ele\n",
        "\n",
        "for key, val in change_dict.items():\n",
        "    if key in resulting_dict80:\n",
        "        resulting_dict80[key] = val\n",
        "for key, val in change_dict.items():\n",
        "    if key in resulting_dict70:\n",
        "        resulting_dict70[key] = val\n",
        "\n",
        "\n",
        "with open(\"dict70.txt\", \"w\", encoding=\"utf-8\") as wr:\n",
        "    for k in sorted(list(resulting_dict70.keys())):\n",
        "        wr.write(f\"{k}:{resulting_dict[k]}\\n\")\n",
        "with open(\"dict80.txt\", \"w\", encoding=\"utf-8\") as wr:\n",
        "    for k in sorted(list(resulting_dict80.keys())):\n",
        "        if k in resulting_dict70:\n",
        "            continue\n",
        "        wr.write(f\"{k}:{resulting_dict[k]}\\n\")\n",
        "with open(\"dict95.txt\", \"w\", encoding=\"utf-8\") as wr:\n",
        "    for k in sorted(list(resulting_dict.keys())):\n",
        "        if k in resulting_dict70 or k in resulting_dict80:\n",
        "            continue\n",
        "        wr.write(f\"{k}:{resulting_dict[k]}\\n\")\n",
        "if COLAB:\n",
        "    files.download(\"dict80.txt\")\n",
        "    files.download(\"dict70.txt\")\n",
        "    files.download(\"dict95.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emOU6b2REcr"
      },
      "source": [
        "# generate wordlists\n",
        "wl70 = sorted(list(resulting_dict70.keys()))\n",
        "wl80 = sorted(list(resulting_dict80.keys()))\n",
        "wl95 = sorted(list(resulting_dict.keys()))\n",
        "\n",
        "\n",
        "def write_list(filename, datalist):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as wr:\n",
        "        wr.write(\"\\n\".join(datalist))\n",
        "\n",
        "\n",
        "write_list(\"wordlist70.txt\", wl70)\n",
        "write_list(\"wordlist80.txt\", wl80)\n",
        "write_list(\"wordlist95.txt\", wl95)\n",
        "\n",
        "if COLAB:\n",
        "    files.download(\"wordlist70.txt\")\n",
        "    files.download(\"wordlist80.txt\")\n",
        "    files.download(\"wordlist95.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGuTk2GDlj0D"
      },
      "source": [
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SMn8bMSU9Zu"
      },
      "source": [
        "shutil.rmtree(\"eigolingo/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}